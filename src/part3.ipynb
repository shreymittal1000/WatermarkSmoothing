{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abJuxKOhHh66",
        "outputId": "8c574563-0d52-4c4c-b6bb-58901ba40f7d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: huggingface_hub==0.24.6 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (0.24.6)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 3)) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (4.44.2)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 5)) (1.26.20)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r /content/requirements.txt (line 4)) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r /content/requirements.txt (line 4)) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->-r /content/requirements.txt (line 3)) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->-r /content/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        " !pip3 install -r /content/requirements.txt\n",
        " !pip3 install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (pipeline, AutoTokenizer, AutoModelForCausalLM)\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tools import tensor_rank_positions, rank_difference, n_smaller, z_score\n",
        "from watermark_tools_context_independent import (\n",
        "    generate_soft_greenlist_watermark_context_independent, watermark_checker, predict_greenlist_confidence, smoothed_logits\n",
        ")\n",
        "\n",
        "import os\n",
        "import transformers\n",
        "from huggingface_hub import login\n",
        "import bitsandbytes as bnb\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_Y0zvp5KHmBY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48089f4-05cb-4ebd-c7bc-a5dc908fa0c5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:bitsandbytes.cextension:The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_id: str):\n",
        "    # Set Hugging Face Hub token\n",
        "    hf_token = \"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\"\n",
        "\n",
        "    # Manually login with the token\n",
        "    login(token=hf_token)\n",
        "\n",
        "    if os.path.exists(model_id):\n",
        "        local_dir = \"./\" + model_id\n",
        "        tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
        "        model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", load_in_8bit=True)\n",
        "        return model, tokenizer\n",
        "    else:\n",
        "        print(f\"Model {model_id} not found locally. Downloading and caching/...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            load_in_8bit=True,\n",
        "        )\n",
        "        model.save_pretrained(model_id)\n",
        "        tokenizer.save_pretrained(model_id)\n",
        "        return model, tokenizer"
      ],
      "metadata": {
        "id": "yDFfr0HlHpVX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks_small = torch.load('ranks_small.pt')\n",
        "ranks_large = torch.load('ranks_big.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRqxkwpHs0S",
        "outputId": "f8e6396b-b306-4c80-c923-f7d1663b3dfb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9d8467c9f439>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ranks_small = torch.load('ranks_small.pt')\n",
            "<ipython-input-4-9d8467c9f439>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ranks_large = torch.load('ranks_big.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ranks_small.shape)\n",
        "print(ranks_large.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy61u5NnHxRW",
        "outputId": "b12082c3-e7cf-4921-f9e4-3dcaafb9fd64"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 32000])\n",
            "torch.Size([30, 32000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rank_diff = rank_difference(ranks_large, ranks_small)\n",
        "print(rank_diff)\n",
        "\n",
        "confidence = predict_greenlist_confidence(ranks_large, ranks_small)\n",
        "print(confidence)\n",
        "print(torch.max(confidence))\n",
        "print(torch.min(confidence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I6_Jb4eI0C5",
        "outputId": "f339527c-eb85-4b82-861a-0514114718da"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5314,    -8, 21325,  ...,   -23,   -23,   -23],\n",
            "        [ 5301,   -14,   -14,  ...,   -28,   -28,   -28],\n",
            "        [ 5301,   -14,   -14,  ...,   -29,   -29,   -29],\n",
            "        ...,\n",
            "        [  -26,   -13, 21325,  ...,   -29,   -29,   -29],\n",
            "        [  -28,   -15, 21299,  ...,   -29,   -29,   -29],\n",
            "        [ 5301,   -13, 21325,  ...,   -27,   -27,   -27]])\n",
            "tensor([0.7754, 0.6890, 0.8013,  ..., 0.4954, 0.4954, 0.4954],\n",
            "       dtype=torch.float16)\n",
            "tensor(0.9971, dtype=torch.float16)\n",
            "tensor(0.0006, dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(rank_diff, 'rank_diff.pt')\n",
        "torch.save(confidence, 'confidence.pt')"
      ],
      "metadata": {
        "id": "RmfhIOa7PuNK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_unclean = torch.load('logits_big.pt')\n",
        "small_unclean = torch.load('logits_small.pt')\n",
        "logits_large = torch.stack([large_unclean[i][0] for i in range(len(large_unclean))])\n",
        "logits_small = torch.stack([small_unclean[i][0] for i in range(len(small_unclean))])"
      ],
      "metadata": {
        "id": "Kzfp5cwoVqXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "860307f6-4e03-4810-d88e-f4e05af35314"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b54ae3309da7>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  large_unclean = torch.load('logits_big.pt')\n",
            "<ipython-input-8-b54ae3309da7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  small_unclean = torch.load('logits_small.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits_large)\n",
        "print(logits_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsLAvUjpdE47",
        "outputId": "184e28ea-40b0-4c43-c657-ef8232e1a0d3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])\n",
            "tensor([[  -inf,   -inf, 9.2500,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n",
            "        ...,\n",
            "        [  -inf,   -inf, 8.6719,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf, 7.1367,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf, 8.7266,  ...,   -inf,   -inf,   -inf]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smoothed_logits = smoothed_logits(confidence, logits_large, logits_small)\n",
        "torch.save(smoothed_logits, 'smoothed_logits.pt')"
      ],
      "metadata": {
        "id": "bByNMxrNWBzQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(smoothed_logits)\n",
        "print(smoothed_logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRoYzYi3epwY",
        "outputId": "0e0c8b8e-c7ca-4f56-c4b9-b0587b4f6f99"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])\n",
            "torch.Size([30, 32000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model for LLaMA 3.1 (use the appropriate model ID)\n",
        "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n",
        "model, tokenizer = load_model(model_id)\n",
        "LLAMA_VOCAB_SIZE = tokenizer.vocab_size\n",
        "print(LLAMA_VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 581
        },
        "id": "dI7Q9_0Ba_Ea",
        "outputId": "aae7683f-ab4d-4d4d-9677-db1a505299fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Model meta-llama/Llama-2-7b-hf not found locally. Downloading and caching/...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "No GPU found. A GPU is needed for quantization.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-9eb0f41d6294>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# tokenizer = AutoTokenizer.from_pretrained(model_id)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mLLAMA_VOCAB_SIZE\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLLAMA_VOCAB_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-1e57851a335d>\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(model_id)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Model {model_id} not found locally. Downloading and caching/...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    562\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mmodel_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_model_mapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 564\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    565\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    566\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   3396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3397\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3398\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   3399\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfrom_flax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_flax\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3400\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mvalidate_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No GPU found. A GPU is needed for quantization.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_accelerate_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: No GPU found. A GPU is needed for quantization."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad token (instruct models usually use the same eos token as pad token)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Prepare the input\n",
        "inputs = tokenizer([\"Who is the current president of the USA? The current president of the United States of America \"], return_tensors=\"pt\")\n",
        "\n",
        "# Generate tokens using Greedy Search and retrieve the scores for each generated token\n",
        "outputs = model.generate(**inputs, max_new_tokens=30, return_dict_in_generate=True, output_scores=True, do_sample=True)\n",
        "outputs.scores = smoothed_logits\n",
        "\n",
        "# Calculate the transition scores (logits to probabilities) for each generated token\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, normalize_logits=True\n",
        ")\n",
        "\n",
        "# Calculate the input length (relevant for decoder-only models like LLaMA)\n",
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "\n",
        "# Extract the generated tokens after the input prompt\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "\n",
        "# Print the model output\n",
        "print(tokenizer.decode(generated_tokens[0]))\n",
        "\n",
        "# Print the generated tokens and their transition scores (converted to probabilities)\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    token = tokenizer.decode(tok)  # Decode token id back to string\n",
        "    print(f\"Generated token: {token}, Probability score: {np.exp(score.item())}\")"
      ],
      "metadata": {
        "id": "TYAe-QGMbSCH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}