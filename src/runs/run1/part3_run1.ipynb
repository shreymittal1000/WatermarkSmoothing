{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9791d3ecf41d4267849fda235604fc40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b886afb0c1854293b1939f4b94b9674a",
              "IPY_MODEL_556df4e67a2c41a2a18b24851719dadc",
              "IPY_MODEL_9a77790fc49a43f981e4e26c7642ebd0"
            ],
            "layout": "IPY_MODEL_2240a98c89b44b4d99c5870f896b5b94"
          }
        },
        "b886afb0c1854293b1939f4b94b9674a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f727ff1f651c45feb8d0fe0e0c1b55dc",
            "placeholder": "​",
            "style": "IPY_MODEL_38745477af07492293b84560f0d59b88",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "556df4e67a2c41a2a18b24851719dadc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_005f52c83253446a833dcce9ea076330",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aee1c12499f840ba955dc64cff841b61",
            "value": 2
          }
        },
        "9a77790fc49a43f981e4e26c7642ebd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cf15d62863614d69a19ec0eb6c153f7b",
            "placeholder": "​",
            "style": "IPY_MODEL_718d48e283ac4f128e0ab8dc56e08711",
            "value": " 2/2 [00:39&lt;00:00, 18.35s/it]"
          }
        },
        "2240a98c89b44b4d99c5870f896b5b94": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f727ff1f651c45feb8d0fe0e0c1b55dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38745477af07492293b84560f0d59b88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "005f52c83253446a833dcce9ea076330": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aee1c12499f840ba955dc64cff841b61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cf15d62863614d69a19ec0eb6c153f7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "718d48e283ac4f128e0ab8dc56e08711": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abJuxKOhHh66",
        "outputId": "e2707716-3908-40a9-9f27-51e56a8e9f66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (0.34.2)\n",
            "Requirement already satisfied: huggingface_hub==0.24.6 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (0.24.6)\n",
            "Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 3)) (2.4.1+cu121)\n",
            "Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (4.44.2)\n",
            "Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 5)) (1.26.20)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (6.0.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (0.4.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2024.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r /content/requirements.txt (line 4)) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r /content/requirements.txt (line 4)) (0.19.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->-r /content/requirements.txt (line 3)) (3.0.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->-r /content/requirements.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.44.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        " !pip3 install -r /content/requirements.txt\n",
        " !pip3 install bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (pipeline, AutoTokenizer, AutoModelForCausalLM)\n",
        "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
        "from torch import Tensor\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from tools import tensor_rank_positions, rank_difference, n_smaller, z_score\n",
        "from watermark_tools_context_independent import (\n",
        "    generate_soft_greenlist_watermark_context_independent, watermark_checker, predict_greenlist_confidence, smoothed_logits\n",
        ")\n",
        "\n",
        "import os\n",
        "import transformers\n",
        "from huggingface_hub import login\n",
        "import bitsandbytes as bnb\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "_Y0zvp5KHmBY"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_id: str):\n",
        "    # Set Hugging Face Hub token\n",
        "    hf_token = \"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\"\n",
        "\n",
        "    # Manually login with the token\n",
        "    login(token=hf_token)\n",
        "\n",
        "    if os.path.exists(model_id):\n",
        "        local_dir = \"./\" + model_id\n",
        "        tokenizer = AutoTokenizer.from_pretrained(local_dir)\n",
        "        model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", load_in_8bit=True)\n",
        "        return model, tokenizer\n",
        "    else:\n",
        "        print(f\"Model {model_id} not found locally. Downloading and caching/...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            device_map=\"auto\",\n",
        "            load_in_8bit=True,\n",
        "        )\n",
        "        model.save_pretrained(model_id)\n",
        "        tokenizer.save_pretrained(model_id)\n",
        "        return model, tokenizer"
      ],
      "metadata": {
        "id": "yDFfr0HlHpVX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranks_small = torch.load('ranks_small.pt')\n",
        "ranks_large = torch.load('ranks_big.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MjRqxkwpHs0S",
        "outputId": "44fdace7-4edc-4e48-c514-edfdfc41ebe2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-9d8467c9f439>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ranks_small = torch.load('ranks_small.pt')\n",
            "<ipython-input-4-9d8467c9f439>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  ranks_large = torch.load('ranks_big.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ranks_small.shape)\n",
        "print(ranks_large.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hy61u5NnHxRW",
        "outputId": "faa89bd3-5d10-4b53-eee5-1676c6f6bc4a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([30, 32000])\n",
            "torch.Size([30, 32000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rank_diff = rank_difference(ranks_large, ranks_small)\n",
        "print(rank_diff)\n",
        "\n",
        "confidence = predict_greenlist_confidence(ranks_large, ranks_small)\n",
        "print(confidence)\n",
        "print(torch.max(confidence))\n",
        "print(torch.min(confidence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1I6_Jb4eI0C5",
        "outputId": "1eb8c49a-a5c4-4e45-8c02-aa11947f9992"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 5310,    -9, 21309,  ...,   -25,   -25,   -25],\n",
            "        [ 5310,    -9,    -9,  ...,   -25,   -25,   -25],\n",
            "        [ 5307,   -10, 21323,  ...,   -25,   -25,   -25],\n",
            "        ...,\n",
            "        [ 5297,   -16,   -16,  ...,   -31,   -31,   -31],\n",
            "        [ 5299,   -14,   -14,  ...,   -29,   -29,   -29],\n",
            "        [  -26,   -13,   -13,  ...,   -24,   -24,   -24]])\n",
            "tensor([0.8911, 0.6821, 0.8130,  ..., 0.4856, 0.4856, 0.4856],\n",
            "       dtype=torch.float16)\n",
            "tensor(0.9971, dtype=torch.float16)\n",
            "tensor(0.0005, dtype=torch.float16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(rank_diff, 'rank_diff.pt')\n",
        "torch.save(confidence, 'confidence.pt')"
      ],
      "metadata": {
        "id": "RmfhIOa7PuNK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "large_unclean = torch.load('logits_big.pt')\n",
        "small_unclean = torch.load('logits_small.pt')\n",
        "logits_large = torch.stack([large_unclean[i][0] for i in range(len(large_unclean))])\n",
        "logits_small = torch.stack([small_unclean[i][0] for i in range(len(small_unclean))])"
      ],
      "metadata": {
        "id": "Kzfp5cwoVqXU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "970ac97c-02f9-4cea-8e41-19f357233dd9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-b54ae3309da7>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  large_unclean = torch.load('logits_big.pt')\n",
            "<ipython-input-8-b54ae3309da7>:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  small_unclean = torch.load('logits_small.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(logits_large)\n",
        "print(logits_small)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsLAvUjpdE47",
        "outputId": "3c956092-7ebb-48b1-ee13-9e55308d9a34"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])\n",
            "tensor([[  -inf,   -inf, 6.6953,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf, 8.1016,  ...,   -inf,   -inf,   -inf],\n",
            "        ...,\n",
            "        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf],\n",
            "        [  -inf,   -inf,   -inf,  ...,   -inf,   -inf,   -inf]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "smoothed_logits = smoothed_logits(confidence, logits_large, logits_small)\n",
        "torch.save(smoothed_logits, 'smoothed_logits.pt')"
      ],
      "metadata": {
        "id": "bByNMxrNWBzQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(smoothed_logits)\n",
        "print(smoothed_logits.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRoYzYi3epwY",
        "outputId": "401309bf-d3e6-41f1-9d34-a85a0139ce86"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [-inf, -inf, -inf,  ..., -inf, -inf, -inf]])\n",
            "torch.Size([30, 32000])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the tokenizer and model for LLaMA 3.1 (use the appropriate model ID)\n",
        "# model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n",
        "model, tokenizer = load_model(model_id)\n",
        "LLAMA_VOCAB_SIZE = tokenizer.vocab_size\n",
        "print(LLAMA_VOCAB_SIZE)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225,
          "referenced_widgets": [
            "9791d3ecf41d4267849fda235604fc40",
            "b886afb0c1854293b1939f4b94b9674a",
            "556df4e67a2c41a2a18b24851719dadc",
            "9a77790fc49a43f981e4e26c7642ebd0",
            "2240a98c89b44b4d99c5870f896b5b94",
            "f727ff1f651c45feb8d0fe0e0c1b55dc",
            "38745477af07492293b84560f0d59b88",
            "005f52c83253446a833dcce9ea076330",
            "aee1c12499f840ba955dc64cff841b61",
            "cf15d62863614d69a19ec0eb6c153f7b",
            "718d48e283ac4f128e0ab8dc56e08711"
          ]
        },
        "id": "dI7Q9_0Ba_Ea",
        "outputId": "23d003bb-1b30-4fe1-e654-b6115b9498c6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
            "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n",
            "  warnings.warn(warning_msg)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9791d3ecf41d4267849fda235604fc40"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "32000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set pad token (instruct models usually use the same eos token as pad token)\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# Prepare the input\n",
        "inputs = tokenizer([\"The president of the United States of America in 2022 is\"], return_tensors=\"pt\")\n",
        "\n",
        "# Generate tokens using Greedy Search and retrieve the scores for each generated token\n",
        "outputs = model.generate(**inputs, max_new_tokens=30, return_dict_in_generate=True, output_scores=True)\n",
        "print(outputs.scores[0].shape)\n",
        "print(outputs.scores[1].shape)\n",
        "print(len(outputs.scores))\n",
        "print(smoothed_logits.shape)\n",
        "outputs.scores = tuple(logit.unsqueeze(0) for logit in smoothed_logits)\n",
        "print(outputs.scores[0].shape)\n",
        "print(len(outputs.scores))\n",
        "\n",
        "\n",
        "\n",
        "# print(outputs.scores)\n",
        "# scores = list(outputs.scores)\n",
        "# for i in range(30):\n",
        "#     scores[i] = smoothed_logits[]\n",
        "# outputs.scores = tuple(scores)\n",
        "\n",
        "\n",
        "# Calculate the transition scores (logits to probabilities) for each generated token\n",
        "transition_scores = model.compute_transition_scores(\n",
        "    outputs.sequences, outputs.scores, normalize_logits=True\n",
        ")\n",
        "\n",
        "# Calculate the input length (relevant for decoder-only models like LLaMA)\n",
        "input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n",
        "\n",
        "# Extract the generated tokens after the input prompt\n",
        "generated_tokens = outputs.sequences[:, input_length:]\n",
        "\n",
        "transition_scores = torch.nan_to_num(transition_scores, nan=0.0)\n",
        "\n",
        "\n",
        "# Print the model output\n",
        "print(tokenizer.decode(generated_tokens[0]))\n",
        "\n",
        "# Print the generated tokens and their transition scores (converted to probabilities)\n",
        "for tok, score in zip(generated_tokens[0], transition_scores[0]):\n",
        "    token = tokenizer.decode(tok)  # Decode token id back to string\n",
        "    print(f\"Generated token: {token}, Probability score: {np.exp(score.item())}\")"
      ],
      "metadata": {
        "id": "TYAe-QGMbSCH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78ff3dba-9cee-42f3-f38b-720ca8c24f15"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 32000])\n",
            "torch.Size([1, 32000])\n",
            "30\n",
            "torch.Size([30, 32000])\n",
            "torch.Size([1, 32000])\n",
            "30\n",
            "Joe Biden.\n",
            "The president of the United States of America in 2023 will be Joe Biden.\n",
            "The president of the\n",
            "Generated token: Joe, Probability score: 1.0\n",
            "Generated token: B, Probability score: 1.0\n",
            "Generated token: iden, Probability score: 1.0\n",
            "Generated token: ., Probability score: 1.0\n",
            "Generated token: \n",
            ", Probability score: 0.9767447274866707\n",
            "Generated token: The, Probability score: 0.0\n",
            "Generated token: president, Probability score: 1.1766785635032432e-06\n",
            "Generated token: of, Probability score: 1.0\n",
            "Generated token: the, Probability score: 1.0\n",
            "Generated token: United, Probability score: 1.0\n",
            "Generated token: States, Probability score: 1.0\n",
            "Generated token: of, Probability score: 1.0\n",
            "Generated token: America, Probability score: 1.0\n",
            "Generated token: in, Probability score: 1.0\n",
            "Generated token: , Probability score: 1.0\n",
            "Generated token: 2, Probability score: 1.0\n",
            "Generated token: 0, Probability score: 1.0\n",
            "Generated token: 2, Probability score: 1.0\n",
            "Generated token: 3, Probability score: 1.0\n",
            "Generated token: will, Probability score: 1.0\n",
            "Generated token: be, Probability score: 1.0\n",
            "Generated token: Joe, Probability score: 1.0\n",
            "Generated token: B, Probability score: 1.0\n",
            "Generated token: iden, Probability score: 1.0\n",
            "Generated token: ., Probability score: 0.0\n",
            "Generated token: \n",
            ", Probability score: 0.0\n",
            "Generated token: The, Probability score: 1.0\n",
            "Generated token: president, Probability score: 0.0\n",
            "Generated token: of, Probability score: 1.0\n",
            "Generated token: the, Probability score: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "watermark = torch.load('/content/watermark.pt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX7sDa4hC6AM",
        "outputId": "71519ed4-2e64-467d-fafa-0f0975c7068e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-25-0d6d1714dacf>:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  watermark = torch.load('/content/watermark.pt')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'watermark shape = {watermark.shape}')\n",
        "print(f'generated shape = {generated_tokens.shape}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXxWS3ygDodZ",
        "outputId": "e9f61007-b05b-406b-e2dd-93d95d381149"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "watermark shape = torch.Size([32000])\n",
            "generated shape = torch.Size([1, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "value = watermark_checker(watermark, generated_tokens[0], 0.7)"
      ],
      "metadata": {
        "id": "kbXa5v0QhG_4"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(value)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7lPxQlIDgrD",
        "outputId": "d6b2fd34-dd52-4dcb-cd6f-98ff2eb66c47"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.inf - torch.inf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gcJfxyLKI27_",
        "outputId": "ff277f54-2589-4260-8e32-119b7fb8808f"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "nan"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "44MACH_zKpai"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}