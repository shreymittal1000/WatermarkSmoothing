{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"0c437344ca7a4d19afd4e8eb2f276a13":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e0c1bb4295f349e5a0001a04bf9de65b","IPY_MODEL_9c6c845253174fca83af6084acefd304","IPY_MODEL_c36fd59691564e25a1ae44f330f42519"],"layout":"IPY_MODEL_66c05156fcc24ac9943d0a81637715f5"}},"e0c1bb4295f349e5a0001a04bf9de65b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_58077fc9abd24a62a8fd6f16793ce9cd","placeholder":"​","style":"IPY_MODEL_5a83747576ce459dadbb6f9f167d615e","value":"Loading checkpoint shards: 100%"}},"9c6c845253174fca83af6084acefd304":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a584f6a49aa84772b026d26e705cf5bc","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e4f25b056428428abef0e62a54fb64fd","value":2}},"c36fd59691564e25a1ae44f330f42519":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_61dd0eb1af024ec48d1c8a1684ca37ee","placeholder":"​","style":"IPY_MODEL_26d90b13e3064b97b3ae12bdef7e9c05","value":" 2/2 [00:59&lt;00:00, 28.77s/it]"}},"66c05156fcc24ac9943d0a81637715f5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58077fc9abd24a62a8fd6f16793ce9cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5a83747576ce459dadbb6f9f167d615e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a584f6a49aa84772b026d26e705cf5bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e4f25b056428428abef0e62a54fb64fd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"61dd0eb1af024ec48d1c8a1684ca37ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26d90b13e3064b97b3ae12bdef7e9c05":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HMdiCq7kCrP1","outputId":"8c6991e6-1e60-4f90-e7ce-8f432e59e76f","executionInfo":{"status":"ok","timestamp":1727008632783,"user_tz":-120,"elapsed":16683,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: accelerate==0.34.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 1)) (0.34.2)\n","Requirement already satisfied: huggingface_hub==0.24.6 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 2)) (0.24.6)\n","Requirement already satisfied: torch==2.4.1 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 3)) (2.4.1+cu121)\n","Requirement already satisfied: transformers==4.44.2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 4)) (4.44.2)\n","Requirement already satisfied: urllib3<2 in /usr/local/lib/python3.10/dist-packages (from -r /content/requirements.txt (line 5)) (1.26.20)\n","Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (24.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (6.0.2)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.34.2->-r /content/requirements.txt (line 1)) (0.4.5)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.16.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2024.6.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (4.66.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.4.1->-r /content/requirements.txt (line 3)) (3.1.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r /content/requirements.txt (line 4)) (2024.9.11)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.44.2->-r /content/requirements.txt (line 4)) (0.19.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.4.1->-r /content/requirements.txt (line 3)) (2.1.5)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub==0.24.6->-r /content/requirements.txt (line 2)) (2024.8.30)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.4.1->-r /content/requirements.txt (line 3)) (1.3.0)\n","Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.43.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.4.1+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.3)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.6.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (2.1.5)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->bitsandbytes) (1.3.0)\n"]}],"source":[" !pip3 install -r /content/requirements.txt\n"," !pip3 install bitsandbytes"]},{"cell_type":"code","source":["from transformers import (pipeline, AutoTokenizer, AutoModelForCausalLM)\n","from transformers.modeling_outputs import CausalLMOutputWithPast\n","from torch import Tensor\n","import torch\n","import torch.nn.functional as F\n","from tools import tensor_rank_positions, rank_difference, n_bigger, z_score\n","from watermark_tools_context_independent import (\n","    generate_soft_greenlist_watermark_context_independent, watermark_checker, predict_greenlist_confidence\n",")"],"metadata":{"id":"GWAcarKIDFJv","executionInfo":{"status":"ok","timestamp":1727008646686,"user_tz":-120,"elapsed":13906,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["import os\n","import torch\n","import transformers\n","from huggingface_hub import login\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","import bitsandbytes as bnb\n","\n","def load_model(model_id: str):\n","    # Set Hugging Face Hub token\n","    hf_token = \"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\"\n","\n","    # Manually login with the token\n","    login(token=hf_token)\n","\n","    if os.path.exists(model_id):\n","        local_dir = \"./\" + model_id\n","        tokenizer = AutoTokenizer.from_pretrained(local_dir)\n","        model = AutoModelForCausalLM.from_pretrained(local_dir, device_map=\"auto\", load_in_8bit=True)\n","        return model, tokenizer\n","    else:\n","        print(f\"Model {model_id} not found locally. Downloading and caching/...\")\n","        tokenizer = AutoTokenizer.from_pretrained(model_id)\n","        model = AutoModelForCausalLM.from_pretrained(\n","            model_id,\n","            device_map=\"auto\",\n","            load_in_8bit=True,\n","        )\n","        model.save_pretrained(model_id)\n","        tokenizer.save_pretrained(model_id)\n","        return model, tokenizer"],"metadata":{"id":"OhSnJtkbI7hp","executionInfo":{"status":"ok","timestamp":1727008646687,"user_tz":-120,"elapsed":16,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"_bEdetpm6H0S","executionInfo":{"status":"ok","timestamp":1727008646687,"user_tz":-120,"elapsed":14,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S3TwgQqqyq0X","outputId":"4cb9628c-5de8-41ea-c177-cfea6d83b74d","executionInfo":{"status":"ok","timestamp":1727008647068,"user_tz":-120,"elapsed":395,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Sep 22 12:37:25 2024       \n","+---------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n","|-----------------------------------------+----------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                      |               MIG M. |\n","|=========================================+======================+======================|\n","|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n","| N/A   42C    P8               9W /  70W |      3MiB / 15360MiB |      0%      Default |\n","|                                         |                      |                  N/A |\n","+-----------------------------------------+----------------------+----------------------+\n","                                                                                         \n","+---------------------------------------------------------------------------------------+\n","| Processes:                                                                            |\n","|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n","|        ID   ID                                                             Usage      |\n","|=======================================================================================|\n","|  No running processes found                                                           |\n","+---------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM\n","import numpy as np\n","\n","# Load the tokenizer and model for LLaMA 3.1 (use the appropriate model ID)\n","model_id = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n","# tokenizer = AutoTokenizer.from_pretrained(model_id)\n","# model = AutoModelForCausalLM.from_pretrained(model_id, load_in_8bit=True, device_map=\"auto\")\n","model, tokenizer = load_model(model_id)\n","LLAMA_3.1_VOCAB_SIZE = tokenizer.vocab_size\n","print(LLAMA_3.1_VOCAB_SIZE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":212,"referenced_widgets":["0c437344ca7a4d19afd4e8eb2f276a13","e0c1bb4295f349e5a0001a04bf9de65b","9c6c845253174fca83af6084acefd304","c36fd59691564e25a1ae44f330f42519","66c05156fcc24ac9943d0a81637715f5","58077fc9abd24a62a8fd6f16793ce9cd","5a83747576ce459dadbb6f9f167d615e","a584f6a49aa84772b026d26e705cf5bc","e4f25b056428428abef0e62a54fb64fd","61dd0eb1af024ec48d1c8a1684ca37ee","26d90b13e3064b97b3ae12bdef7e9c05"]},"id":"78Sotw0mWbnd","executionInfo":{"status":"ok","timestamp":1727008708197,"user_tz":-120,"elapsed":61138,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}},"outputId":"d835ff89-6ad0-417e-9aef-4b983f176c2f"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n","Token is valid (permission: fineGrained).\n","Your token has been saved to /root/.cache/huggingface/token\n","Login successful\n"]},{"output_type":"stream","name":"stderr","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n","Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n","/usr/local/lib/python3.10/dist-packages/transformers/quantizers/auto.py:174: UserWarning: You passed `quantization_config` or equivalent parameters to `from_pretrained` but the model you're loading already has a `quantization_config` attribute. The `quantization_config` from the model will be used.\n","  warnings.warn(warning_msg)\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0c437344ca7a4d19afd4e8eb2f276a13"}},"metadata":{}}]},{"cell_type":"code","source":["# Generate Watermark\n","watermark = generate_soft_greenlist_watermark_context_independent(\n","    LLAMA_3.1_VOCAB_SIZE,\n","    0.5\n","    50,\n",")"],"metadata":{"id":"F-5aeP_c3kqV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set pad token (instruct models usually use the same eos token as pad token)\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","\n","# Prepare the input\n","inputs = tokenizer([\"Who is the current president of the USA?\"], return_tensors=\"pt\")\n","\n","# Generate tokens using Greedy Search and retrieve the scores for each generated token\n","outputs = model.generate(**inputs, max_new_tokens=20, return_dict_in_generate=True, output_scores=True, do_sample=True)\n","\n","for i in range(20):\n","    outputs.scores[i] += watermark\n","\n","# Calculate the transition scores (logits to probabilities) for each generated token\n","transition_scores = model.compute_transition_scores(\n","    outputs.sequences, outputs.scores, normalize_logits=True\n",")\n","\n","# Calculate the input length (relevant for decoder-only models like LLaMA)\n","input_length = 1 if model.config.is_encoder_decoder else inputs.input_ids.shape[1]\n","\n","# Extract the generated tokens after the input prompt\n","generated_tokens = outputs.sequences[:, input_length:]\n","\n","# Print the generated tokens and their transition scores (converted to probabilities)\n","# for tok, score in zip(generated_tokens[0], transition_scores[0]):\n","#     token = tokenizer.decode(tok)  # Decode token id back to string\n","#     print(f\"Generated token: {token}, Probability score: {np.exp(score.item())}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UgPauDKpZ8-D","executionInfo":{"status":"ok","timestamp":1727010580958,"user_tz":-120,"elapsed":7432,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}},"outputId":"721e7d71-67cf-4284-9310-65b1736fbf22"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n","/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1885: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["torch.Size([1, 128256])\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"II-FhGWKm00Y","executionInfo":{"status":"ok","timestamp":1727008714660,"user_tz":-120,"elapsed":16,"user":{"displayName":"Shrey Mittal","userId":"02553658435765811888"}}},"execution_count":7,"outputs":[]}]}